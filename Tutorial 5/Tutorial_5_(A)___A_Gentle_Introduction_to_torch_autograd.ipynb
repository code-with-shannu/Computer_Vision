{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EjhcYiYZq5d"
      },
      "source": [
        "A Gentle Introduction to `torch.autograd`\n",
        "=========================================\n",
        "\n",
        "`torch.autograd` is PyTorch's automatic differentiation engine that\n",
        "powers neural network training. In this section, you will get a\n",
        "conceptual understanding of how autograd helps a neural network train.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnBSh4OdZq5g"
      },
      "source": [
        "Differentiation in Autograd\n",
        "===========================\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create tensors.\n",
        "x = torch.tensor(3.)\n",
        "m = torch.tensor(4., requires_grad=True)\n",
        "c = torch.tensor(5., requires_grad=True)"
      ],
      "metadata": {
        "id": "Fst-oG8JcIhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,m,c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrwzN42a8cPi",
        "outputId": "1d4bb769-e42d-467f-8c86-dc1f79e9fa97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can combine tensors with the usual arithmetic operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "tE7QxqkYcUze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = m * x + c\n",
        "print(y)"
      ],
      "metadata": {
        "id": "Ytxp8MIQcRuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d8dfd8-0430-4207-e180-852b8d5b8335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(17., grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$y = mx +c$$"
      ],
      "metadata": {
        "id": "DfTp5Vsbcz6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradients\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "pHbWspPKcj_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "999802dc-62d8-41f1-aa5c-d4c449656e33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-50880f130d25>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\frac{d y}{d m} = x$$\n",
        "\n",
        "$$\\frac{d y}{d c} = 1$$"
      ],
      "metadata": {
        "id": "WHTofQOsdC34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display gradients\n",
        "print('dy/dm:', m.grad)\n",
        "print('dy/dc:', c.grad)"
      ],
      "metadata": {
        "id": "OlrhftkIcm6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8fba6c-bf24-4dac-ee7a-f514d5318ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy/dm: tensor(3.)\n",
            "dy/dc: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `autograd` tracks only those tensors for which the `requires_grad=True`."
      ],
      "metadata": {
        "id": "QWxNyW4CdpAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let\\'s now create two tensors `a` and `b` with `requires_grad=True`. This signals to\n",
        "`autograd` that every operation on them should be tracked."
      ],
      "metadata": {
        "id": "zz--rCT0cFLM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEgj2oroZq5g"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjndf1y0Zq5g"
      },
      "source": [
        "We create another tensor `Q` from `a` and `b`.\n",
        "\n",
        "$$Q = 3a^3 - b^2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxoI5IbZZq5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46458fac-81fd-4d7f-e334-d79870f53795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-12.,  65.], grad_fn=<SubBackward0>)\n"
          ]
        }
      ],
      "source": [
        "Q = 3*a**3 - b**2\n",
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzxBtGa1Zq5h"
      },
      "source": [
        "Let\\'s assume `a` and `b` to be parameters of an NN, and `Q` to be the\n",
        "error. In NN training, we want gradients of the error w.r.t. parameters,\n",
        "i.e.\n",
        "\n",
        "$$\\frac{\\partial Q}{\\partial a} = 9a^2$$\n",
        "\n",
        "$$\\frac{\\partial Q}{\\partial b} = -2b$$\n",
        "\n",
        "When we call `.backward()` on `Q`, autograd calculates these gradients\n",
        "and stores them in the respective tensors\\' `.grad` attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q.backward()"
      ],
      "metadata": {
        "id": "r_MNTUXRasff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "77eeaf4f-9171-4942-891d-a67500c16077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "grad can be implicitly created only for scalar outputs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-42395d6027c1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    133\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`grad` can be implicitly created only for scalar outputs. Since $Q$ (or `Q`) is not a scalar we cannot compute gradients over it (*or atleast directly*)."
      ],
      "metadata": {
        "id": "vxYYwSaueGmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In `PyTorch`, when you perform a backward pass using `.backward()`, gradients are computed for scalar outputs by default, which is straightforward for most loss functions used in training neural networks.\n",
        "\n",
        "\n",
        "However, when the output (`Q` in our case) is not a scalar but a vector, `PyTorch` requires an additional argument to `.backward()`, specifying the gradients of the output tensor with respect to some scalar value. This additional argument is `gradient=external_grad` in our example."
      ],
      "metadata": {
        "id": "1CAESsREeyuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vX0dfAAZq5h"
      },
      "outputs": [],
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `external_grad` tensor we are passing as a parameter to `Q.backward(gradient=external_grad)` specifies the gradients of some higher-level scalar with respect to `Q`.\n",
        "\n",
        "Essentially, it tells `PyTorch` how to weight each element of `Q` in the gradient computation, effectively treating the elements of `external_grad` as coefficients for a linear combination of the elements in `Q` that results in a scalar.\n",
        "\n"
      ],
      "metadata": {
        "id": "i0rVHKOMfDaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad, b.grad"
      ],
      "metadata": {
        "id": "6lj8DJroa11Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c08653-a9f6-429f-f728-8cf77d30ba45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([36., 81.]), tensor([-12.,  -8.]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cF61KkPZq5h"
      },
      "source": [
        "Gradients are now deposited in `a.grad` and `b.grad`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_19e3Fg0Zq5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04e9656c-789b-4544-cfcf-a147e88c715d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ],
      "source": [
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equivalently, we can also aggregate Q into a scalar and call backward\n",
        "implicitly, like `Q.sum().backward()`.\n"
      ],
      "metadata": {
        "id": "eImyd_pBg_ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: [A Gentle Introduction to torch.autograd\n",
        "](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
      ],
      "metadata": {
        "id": "u6TqrefXgcwe"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}